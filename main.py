import os
import torch
import matplotlib.pyplot as plt
from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer
from datasets import load_dataset
from transformers import BitsAndBytesConfig, default_data_collator
from peft import get_peft_model, LoraConfig

# âœ… Ensure CUDA Memory Optimization
torch.backends.cuda.matmul.allow_tf32 = True

# âœ… Load Dataset
dataset_name = "squad_v2"
dataset = load_dataset(dataset_name)

# âœ… Load Model & Tokenizer
model_name = "mistralai/Mistral-7B-v0.1"
tokenizer = AutoTokenizer.from_pretrained(model_name)
tokenizer.pad_token = tokenizer.eos_token  

# âœ… Preprocessing Function
def preprocess_function(examples):
    inputs = [q.strip() + " " + c.strip() for q, c in zip(examples["question"], examples["context"])]
    targets = [a["text"][0].strip() if len(a["text"]) > 0 else "unanswerable" for a in examples["answers"]]
    model_inputs = tokenizer(inputs, max_length=512, truncation=True, padding="max_length")
    labels = tokenizer(text_target=targets, max_length=512, truncation=True, padding="max_length")
    labels["input_ids"] = [[(label if label != tokenizer.pad_token_id else -100) for label in seq] for seq in labels["input_ids"]]
    model_inputs["labels"] = labels["input_ids"]
    return model_inputs

# âœ… Apply Tokenization
print("ðŸ“Œ Preprocessing dataset...")
tokenized_dataset = dataset.map(preprocess_function, batched=True, remove_columns=dataset["train"].column_names)
tokenized_dataset.set_format("torch")
print("âœ… Dataset Preprocessed Successfully!")

# âœ… Configure QLoRA for Efficient Training
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype="float16",
    bnb_4bit_use_double_quant=True
)

# âœ… Load Mistral 7B Model
print("ðŸ“Œ Loading Mistral 7B Model...")
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    quantization_config=bnb_config,
    device_map="auto",
    offload_folder="offload"")
print("âœ… Model Loaded Successfully!")

# âœ… Configure LoRA
lora_config = LoraConfig(
    r=8,
    lora_alpha=16,
    lora_dropout=0.1,
    bias="none"")

peft_model = get_peft_model(model, lora_config)
peft_model.config.use_cache = False
peft_model.config.pretraining_tp = 1

# âœ… Define Training Arguments
training_args = TrainingArguments(
    output_dir="./mistral_finetuned",
    per_device_train_batch_size=1,
    gradient_accumulation_steps=8,
    learning_rate=2e-4,
    num_train_epochs=3,
    save_steps=1000,
    evaluation_strategy="steps",
    eval_steps=500,
    logging_dir="./logs",
    logging_steps=50,
    save_total_limit=2,
    load_best_model_at_end=True,
    metric_for_best_model="eval_loss",
    resume_from_checkpoint=True if os.path.exists("./mistral_finetuned") else None
)

# âœ… Custom Trainer Class (Logs Locally)
class CustomTrainer(Trainer):
    def on_log(self, args, state, control, logs=None, **kwargs):
        if logs is not None:
            grad_norm = self._compute_gradient_norm()
            logs["grad_norm"] = grad_norm
            print(f"ðŸ“Š Step {state.global_step} | Loss: {logs.get('loss', 'N/A')} | Val Loss: {logs.get('eval_loss', 'N/A')} | Grad Norm: {grad_norm}")

    def _compute_gradient_norm(self):
        total_norm = 0.0
        for p in self.model.parameters():
            if p.grad is not None:
                param_norm = p.grad.detach().data.norm(2)
                total_norm += param_norm.item() ** 2
        return total_norm ** 0.5

# âœ… Initialize Trainer
trainer = CustomTrainer(
    model=peft_model,
    args=training_args,
    train_dataset=tokenized_dataset["train"],
    eval_dataset=tokenized_dataset["validation"],
    data_collator=default_data_collator
)

# âœ… Start Training ðŸš€
print("ðŸ“Œ Starting Model Training...")
trainer.train()
print("âœ… Training Completed Successfully!")

# âœ… Save Fine-Tuned Model
print("ðŸ“Œ Saving Fine-Tuned Model...")
peft_model.save_pretrained("./mistral_finetuned")
tokenizer.save_pretrained("./mistral_finetuned")
print("ðŸŽ‰ Fine-tuning complete! Model saved in ./mistral_finetuned ðŸŽ‰")

# âœ… Function to Plot Loss & Gradient Norm
def plot_metrics():
    logs = trainer.state.log_history
    steps = [entry["step"] for entry in logs if "loss" in entry]
    train_losses = [entry["loss"] for entry in logs if "loss" in entry]
    eval_losses = [entry["eval_loss"] for entry in logs if "eval_loss" in entry]
    grad_norms = [entry["grad_norm"] for entry in logs if "grad_norm" in entry]

    plt.figure(figsize=(10, 5))
    
    # âœ… Plot Training & Validation Loss
    plt.subplot(1, 2, 1)
    plt.plot(steps, train_losses, label='Training Loss', color='blue')
    plt.plot(steps, eval_losses, label='Validation Loss', color='red')
    plt.xlabel('Steps')
    plt.ylabel('Loss')
    plt.title('Training vs Validation Loss')
    plt.legend()
    
    # âœ… Plot Gradient Norm Stability
    plt.subplot(1, 2, 2)
    plt.plot(steps, grad_norms, label='Gradient Norm', color='green')
    plt.xlabel('Steps')
    plt.ylabel('Gradient Norm')
    plt.title('Gradient Norm Stability')
    plt.legend()

    plt.show()

# âœ… Plot Results After Training
plot_metrics()
